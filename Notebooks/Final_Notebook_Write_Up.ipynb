{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Write-Up ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write-Up Section ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Competiton 2, we found that by completing Competition 1 very thoroughly helped with the beginning of Competition 2. Being able to use nearly all of the same code for our MinMax notebook for preprocessing the data help immensely. As a group, we felt like the best model from our first competition was using the MinMax, so we decided that that was what we were going to use for preprocessing our data.\n",
    "   \n",
    "At the end of our [first notebook](./Pipelines/MinMax1.ipynb), we ran a logistic regression model as our first model. Our results from the logistic regression model were initially good, by having an F1-score of around 79 and an AUC of 70. These scores were better than the scores of our Competition 1 logistic regression, so the results early on look promising. But we did find something different in Competition 2 than in Competition 1. When we did the correlation analysis of the features, we did not find any that were highly correlated with the target feature (`Y`, or if a credit card will default (1) or not (0)). We thought that this was interesting because usually there is going to be at least a highly correlated feature to the target in some datasets, such as in Competition 1, when we had 1. But we did not find any in this dataset. Possibly, if we were to combine features together to eliminate the amount of features, then possibly the newly created features could be highly correlated, but we did not do that. Furthermore, we binned variables X3 through X5, which was Education (X3), Marital Status (X4), and Age (X5). We felt that these would be the best to bin initially. We finally exported the data set with the binned variables and the preprocessing to use in other models.\n",
    "\n",
    "The total amount of models that we created was **6**, including the Logistic Regression model that we created in the [MinMax Notebook](./Pipelines/MinMax1.ipynb).\n",
    "\n",
    "The best model that we created, in our opinion, was the [Random Forest Model](./Random_Forest/Random_Forest1.ipynb). With an F1-Score of at least 80 and an Accuracy of at least 82. We decided to use 100 decision trees for running the model. In our presentation, we learned that we should use more trees the more features we have, and the less amount the less features that we have.\n",
    "\n",
    "The third best model, in our opinion, was the [XGBoost Model](./XGBoost/XG_Boost_1.ipynb). We had used the XGBRegressor package the first time, and had an RMSE of 0.37, but then we changed the package to XGBClassifier, and the RMSE increased to 0.4206, which is not an extremely big change, but we feel like the RMSE is very low to where it should be. But if it is the correct way to create the model, then we have to assume that it is very good.\n",
    "\n",
    "These are the only models that we would like to talk about, as the other models did not seem to perform as well as the others, but here are the links to the other notebooks:\n",
    "\n",
    "- [SVM Notebook](./SVM/SVM1.ipynb)\n",
    "- [Neural Network Notebook](./NeuralNetwork/Neural_Network_Good.ipynb)\n",
    "- [TPot Notebook](./TPot/TPot_Notebook.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Crossing the river over the bride we built\" ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per what Dr. Huntley said about building a bridge (our models) and then crossing the river (being able to use them to make a company more profitable), we feel like that there are certain features that are more important that others. The way that we determined this was to use feature importance in our [XGBoost Notebook](./XGBoost/XG_Boost_1.ipynb). At the bottom of this notebook, you will find the feature importance that we visualized with a horizontal bar chart. What this tells a person reading this is how important certain features are to determining whether a person's credit card is going to default or not. These features can also help determine if a credit card company should increase/decrease a person's credit, depending on the likelihood that their credit card is going to default.\n",
    "\n",
    "The most important features of the data were X21, X9, X14,  X15, X13, and X6. There are no categorical variables out of any of the highest of importance features. To further help understand what could be used as a reason why some people may have a credit card default or not is to bin all of the features, and then rerun the feature importane code and see if there are any changes. These categories after doing the binning will help pinpoint if a credit card is either going to default or not after a certain time depending on the amount of debt that a person accumulates. We can also try to find trends in the data without binning, in which case we can strip out all of the people that do not fall into a certain category, such as credit card not defaulting and seeing their payment due in X6, and find trends that would either lead us to believe that their credit card is going to default or not.\n",
    "\n",
    "This would help a credit card company because it can help detect whether they should increase a person's credit, or take back some of the credit because they are in fear that they would not make payments on time. This could also help the people that have credit cards by showing them the likelihood that their credit card is going to default if they are behind on payments or owe a certain amount of money. Ultimately, this would be used by a credit card company more ofen than not, so they can inform a person if their credit card is on track to default, or if they have to change the way that they issue credit to people based on their debt and payment cycle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
